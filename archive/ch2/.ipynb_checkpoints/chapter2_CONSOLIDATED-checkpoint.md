# Chapter 2: Data Foundations â€” From Raw Events to Regulator-Ready

---

## Executive Summary

**What this chapter covers:**
- Why data quality is existential in financial services (Knight Capital's $440M lesson)
- The four pillars of fit-for-purpose data: Quality, Lineage, Documentation, Privacy
- Building a four-layer data cleaning pipeline: Schema validation â†’ Missing data â†’ Deduplication â†’ Referential integrity
- Creating audit trails and documentation for regulatory compliance
- Hands-on practice with Atlas Bank's messy datasets

**Key takeaways:**
- In regulated finance, data quality isn't optionalâ€”it's mandated (BCBS 239, SR 11-7)
- Every data cleaning decision must be logged, justified, and reproducible
- "Good enough" data for exploration is often dangerously inadequate for decisioning
- Transparency beats clevernessâ€”simple, documented pipelines win

**Time estimate:**
- Path A (Hands-On): 4-6 hours (reading + coding)
- Path B (Conceptual): 2-3 hours (reading only)

**What you'll build:**
- A complete data cleaning pipeline for banking data
- A data quality logger with full audit trail
- Documentation that would satisfy a regulatory examination

---

## 2.1 Problem Framing

### Opening Vignette: The $440 Million Data Quality Problem

In 2012, Knight Capital Groupâ€”one of Wall Street's largest market makersâ€”deployed new trading software without properly testing it against their production data. A configuration error went unnoticed because the data used in testing didn't match the real-world messiness of live market feeds. Within 45 minutes of going live, the faulty software sent millions of erroneous orders, generating a $440 million loss and nearly bankrupting the firm.

The root cause? Knight's test data was too clean. It didn't capture the edge cases, format inconsistencies, and timing quirks present in real market data. When the software encountered these "messy" conditions in production, it failed catastrophically.

This isn't a story about bad codeâ€”it's a story about bad data preparation. The software worked perfectly in testing because the test data had been carefully curated. Production data, as it always is, was different.

> ðŸ’¡ **Key Insight:** In financial technology, data quality isn't a "nice-to-have"â€”it's existential. Bad data doesn't just produce bad analytics; it produces bad decisions, regulatory violations, and in Knight Capital's case, near-total business failure.

---

### 2.1.1 Why Data Quality Isn't Optional in Fintech

#### The Regulatory Imperative

Financial institutions don't get to choose whether their data is high-quality. Regulators mandate it.

**BCBS 239** (Basel Committee on Banking Supervision Principle 239) requires banks to have "strong data aggregation capabilities and risk reporting practices." In plain English: banks must be able to accurately aggregate risk data across the enterprise and report it to regulators on demand. This requires:
- Complete, accurate data
- Clear data lineage (knowing where data came from)
- Documented transformation rules
- Timely availability

**SR 11-7** (Federal Reserve Supervisory Guidance) extends these requirements to model risk management. Any model used for regulatory capital, stress testing, or material business decisions must have:
- Documented data sources
- Quality assessments of input data
- Validation that data is "fit for purpose"
- Ongoing monitoring of data quality

**GDPR and Privacy Regulations** add another layer: personal data must be accurate, kept up-to-date, and processed lawfully. Financial institutions must be able to demonstrate that customer data is correctâ€”not just stored, but actively maintained.

> ðŸ¢ **Practitioner Reality Check**: These aren't abstract requirements. Regulators audit data quality during examinations. Banks have received enforcement actions (fines, restrictions on new business) specifically for data quality failures. One major bank paid $285 million in 2020 partly due to "unsafe and unsound practices" in data governance.

#### The Business Imperative

Beyond compliance, data quality directly impacts profitability:

**Credit decisions**: A credit model is only as good as its training data. If historical default records are incomplete or inaccurate, the model will systematically misjudge risk. The result?
- Too conservative â†’ turn away good customers â†’ lost revenue
- Too liberal â†’ approve bad customers â†’ credit losses

One regional bank discovered that 8% of their "current" loans in the training data were actually 30+ days past dueâ€”they'd been miscoded during a system migration years earlier. The resulting credit model was dangerously optimistic, leading to $50M in unexpected losses.

**Fraud detection**: Fraud models depend on labeled historical fraud cases. But if fraud labels are inconsistent (one analyst codes a case as "fraud," another as "dispute"), the model learns noise instead of signal. Financial institutions report that 15-30% of fraud labels in their historical data are questionable upon review.

**Customer experience**: Data quality failures create customer-facing disasters:
- Duplicate accounts leading to declined transactions
- Incorrect balances causing overdraft fees
- Failed KYC checks blocking legitimate customers

These aren't just operational hiccupsâ€”they're reputation damage that drives customers to competitors.

#### The Scale of the Problem

Industry surveys consistently show:
- Financial institutions estimate **20-30% of their data has quality issues**
- Data quality problems consume **15-20% of operational capacity** (manual fixes, reconciliations)
- **60% of analytics projects fail or underdeliver** due to data quality issues
- Organizations spend **30-40% of their data budgets** on cleaning and remediation

The irony: modern banks generate more data than ever, but struggle to turn it into reliable insights. The problem isn't volumeâ€”it's validity.

---

### 2.1.2 The Four Pillars of Fit-for-Purpose Data

Not all data quality is created equal. A dataset that's "good enough" for exploratory analysis might be dangerously inadequate for regulatory reporting or automated decisioning. The concept of **fit-for-purpose** asks: "Is this data suitable for its intended use?"

For regulated financial systems, we organize data quality around four pillars:

#### Pillar 1: Quality

Quality encompasses the classic dimensions:

| Dimension | Question | Example |
|-----------|----------|---------|
| **Accuracy** | Does the data correctly represent reality? | Transaction amounts match source systems |
| **Completeness** | Are all required fields populated? | No missing account IDs |
| **Consistency** | Do related data elements agree? | Transaction date not before account open date |
| **Timeliness** | Is the data fresh enough? | Fraud detection needs real-time data |
| **Validity** | Do values conform to business rules? | Account types from defined list |

**Example: What "high quality" looks like**

```
âŒ Low Quality:
transaction_id: NULL
account_id: "ACC-12345 "  (trailing space)
date: "15/32/2023"  (invalid date)
amount: "lots"  (non-numeric)

âœ“ High Quality:
transaction_id: "TXN-0000012345"
account_id: "ACC-00012345"
date: "2023-12-15"
amount: -45.50
```

The low-quality version might load into a system without error, but it's unusable for any serious analysis or decisioning.

#### Pillar 2: Lineage

Lineage means knowing the full history of your data: where it came from, how it was transformed, and who touched it.

**Why lineage matters:**

- *Regulatory compliance*: When a regulator asks "How did you calculate this capital requirement?", you need to trace the calculation back through all data transformations to the original source systems.
- *Debugging models*: When a credit model starts producing strange predictions, you need to trace back: Did the input data change?
- *Impact analysis*: If you need to fix a data quality issue retroactively, you must know which downstream reports, models, and decisions were affected.

**Example: Lineage documentation**
```
Field: customer_credit_score
Source: Experian Credit Bureau API
Extracted: 2024-01-15 08:00:00 UTC
Transformations:
  1. Extracted via API call (extract_credit_data.py v2.3)
  2. Validated score range 300-850 (validate_scores.py v1.1)
  3. Joined to account table on SSN (join_customer_data.sql)
  4. Null scores set to 0 for closed accounts (handle_nulls.py v1.0)
Last Updated: 2024-01-15 09:15:23 UTC
Used In: credit_risk_model_v3, monthly_portfolio_report
```

#### Pillar 3: Documentation

Documentation captures the *meaning* and *context* of your dataâ€”the things that aren't obvious from looking at the data itself.

**Data dictionaries** answer:
- What does each field represent?
- What are valid values?
- What does NULL mean for this field?
- What business rules apply?
- What are known limitations?

**Example: The hidden meaning of NULL**

Consider a `last_payment_date` field for loan accounts:
- NULL could mean: "No payment has ever been made" (new loan)
- NULL could mean: "Payment data is missing" (system error)
- NULL could mean: "Not applicable" (account type that doesn't require payments)

Without documentation, an analyst will guess. And they'll probably guess wrong.

> ðŸ¢ **Practitioner Tip**: Document *why* you made decisions, not just *what* you did. Future you (or your successor) needs to understand the trade-offs. "We imputed missing credit scores as 650 (median) because scores were only missing for 2% of accounts and the distribution was symmetric." This beats "Missing scores set to 650."

#### Pillar 4: Privacy

Financial data is inherently sensitive. Data quality processes must respect privacy constraints:

**PII handling**: Personal data must be encrypted, access-controlled, logged, and masked in non-production environments.

**The challenge**: Data quality often requires examining actual values to identify problems. You can't spot a typo in a customer name if it's been hashed. This creates tension between privacy and qualityâ€”you must balance them carefully.

---

### 2.1.3 The Data Lifecycle in Financial Systems

Understanding where data quality issues arise requires understanding the full data lifecycle:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA LIFECYCLE & QUALITY ISSUES                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Stage 1: EVENT GENERATION (Source Systems)                         â”‚
â”‚  Issues: Manual entry errors, system bugs, timezone mismatches      â”‚
â”‚                                                                     â”‚
â”‚  Stage 2: DATA INGESTION (ETL/ELT)                                  â”‚
â”‚  Issues: Network errors, schema changes, encoding problems          â”‚
â”‚                                                                     â”‚
â”‚  Stage 3: DATA CLEANING & TRANSFORMATION  â† THIS CHAPTER            â”‚
â”‚  Issues: Incorrect assumptions, lossy transforms, undocumented      â”‚
â”‚          decisions                                                  â”‚
â”‚                                                                     â”‚
â”‚  Stage 4: DATA ENRICHMENT (External Sources)                        â”‚
â”‚  Issues: Stale data, failed lookups, mismatched granularity         â”‚
â”‚                                                                     â”‚
â”‚  Stage 5: DATA SERVING (Reports, Models, Decisions)                 â”‚
â”‚  Issues: Drift, inconsistency, staleness                            â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The compounding effect**: Issues from early stages compound. A small data entry error becomes a model bug becomes a bad decision becomes a regulatory violation.

**This chapter focuses on Stage 3**: We control this stage. If we do it wellâ€”with documentation, validation, and lineage trackingâ€”we can prevent downstream problems.

---

### 2.1.4 From "Good Enough" to "Regulator-Ready"

Many analysts are comfortable with "good enough" data:
- "I'll just drop records with missing values"
- "These outliers are probably errors, I'll remove them"
- "I'll manually fix this in Excel"

**That doesn't work in regulated financial systems.** Here's why:

| Requirement | What It Means | Regulator-Ready Standard |
|-------------|---------------|-------------------------|
| **Reproducibility** | Can you reproduce a report from 6 months ago? | Everything version-controlled, documented, automated |
| **Justification** | Why did you drop 1000 records? | Every decision has documented rationale |
| **Auditability** | Can you trace every number back? | Complete lineage from raw to final output |
| **Validation** | How do you know data is correct? | Automated tests, reconciliation, monitoring |

---

### 2.1.5 The Cost of Cutting Corners

What happens when organizations skip proper data preparation?

**Scenario 1: The Retroactive Fix**

A bank discovers their credit model has been using incorrect income data for 18 months. Now they must: fix the bug, re-clean 18 months of data, retrain the model, re-score every application, notify regulators, and potentially offer remediation to affected customers.

**Cost**: 6 months of work, $2-3M in consulting fees, regulatory scrutiny.

**Could have been prevented**: Automated data validation would have caught the issue on day 1.

**Scenario 2: The Model That Couldn't Be Validated**

A fintech builds a fraud detection model with excellent test metrics. But when the model risk team tries to validate it, they discover: training data has no lineage, cleaning logic is scattered, key decisions aren't documented.

The model can't be approved for production. Six months of work is unusable.

**Scenario 3: The Regulatory Violation**

An insurance company submits regulatory stress test results. The regulator asks for supporting data. The numbers don't match the original submission, lineage is incomplete, transformations aren't documented.

**Cost**: Enforcement action, $15M fine, restrictions on new business.

---

### Key Principles for This Chapter

As we move into the technical walkthrough, keep these principles in mind:

1. **Transparency over cleverness**: Simple, documented cleaning beats complex, undocumented magic
2. **Validation is not optional**: Every cleaning step should have a validation check
3. **Document why, not just what**: Future readers need to understand your reasoning
4. **Automate everything**: Manual steps can't be audited or reproduced
5. **Preserve lineage**: Every transformation should be logged
6. **Business rules first**: Data quality rules should come from business experts
7. **Test with messy data**: Don't just test with clean dataâ€”use data with realistic problems

> ðŸ’¡ **Reflection Question**: Before moving on, consider your own organization:
> - What would happen if your data pipeline failed for a week?
> - Could you reproduce a report from 6 months ago?
> - Do you know where every field in your reports comes from?

---

## 2.2 Code Walkthrough â€” Building the Data Pipeline

In this section, we'll build a complete data cleaning pipeline for Atlas Bank's messy datasets. We'll work through each step systematically, documenting our decisions and building in auditability from the start.

By the end, you'll have a reproducible pipeline that transforms raw, problematic data into clean, regulator-ready data.

> ðŸŽ“ **For Path B (Conceptual) Readers:** You don't need to run this code. Follow the explanations before and after each code block to understand *what* we're doing and *why*. The results are shown and interpreted for you.

---

### 2.2.1 Meet Our Messy Data

Let's start by loading the three CSV files from Atlas Bank's legacy system: account information, transaction history, and balance snapshots.

```python
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Load the three datasets from the raw folder
accounts = pd.read_csv('synthetic_banking_data/raw/accounts.csv')
transactions = pd.read_csv('synthetic_banking_data/raw/transactions.csv')
balances = pd.read_csv('synthetic_banking_data/raw/balances.csv')

print(f"âœ“ Loaded {len(accounts):,} accounts")
print(f"âœ“ Loaded {len(transactions):,} transactions")
print(f"âœ“ Loaded {len(balances):,} balance records")
```

**Output:**
```
âœ“ Loaded 1,000 accounts
âœ“ Loaded 9,116 transactions
âœ“ Loaded 42,289 balance records
```

#### Systematic Data Quality Assessment

Before we start cleaning, let's create a comprehensive quality report. This systematic approach is crucialâ€”you can't fix what you haven't measured.

```python
def assess_data_quality(df, table_name):
    """
    Performs systematic data quality assessment on a dataframe.
    Returns a report with completeness, uniqueness, and validity metrics.
    """
    print(f"\n{'='*60}")
    print(f"Data Quality Assessment: {table_name.upper()}")
    print(f"{'='*60}\n")
    
    print(f"Total rows: {len(df):,}")
    print(f"Total columns: {len(df.columns)}")
    
    # Completeness check
    print("\n--- Completeness (Missing Values) ---")
    missing = df.isnull().sum()
    missing_pct = (missing / len(df) * 100).round(2)
    completeness_report = pd.DataFrame({
        'Missing_Count': missing,
        'Missing_Percent': missing_pct
    })
    print(completeness_report[completeness_report['Missing_Count'] > 0])
    
    return completeness_report

# Run assessment on all tables
accounts_quality = assess_data_quality(accounts, 'accounts')
transactions_quality = assess_data_quality(transactions, 'transactions')
balances_quality = assess_data_quality(balances, 'balances')
```

**Key findings from this assessment:**
- **Accounts**: 4.1% missing account_ids (41 records), 60 duplicate account_ids, 9.2% missing branch_code
- **Transactions**: 1.85% missing transaction_ids, ~45% missing merchant data
- **Balances**: Date columns stored as 'object' type instead of datetime

> ðŸ’¡ **Understanding "Duplicates":** When we say "60 duplicate account_ids," we mean there are 60 rows that share an account_id with another row. This is a problem because account_id should be a unique identifier (primary key). Seeing "duplicates" in columns like `account_type` or `status` is normalâ€”many accounts can have the same type!

---

### 2.2.2 The DataQualityLogger

First, we need a way to track every change we make. This is critical for regulatory compliance and debugging.

```python
class DataQualityLogger:
    """
    Tracks all data quality issues and transformations for complete audit trail.
    Every cleaning operation should be logged to maintain lineage.
    """
    
    def __init__(self):
        self.issues = []
        
    def log_issue(self, table, column, issue_type, count, action, reason):
        """
        Log a data quality issue and the action taken.
        
        Args:
            table: Name of the table (accounts, transactions, balances)
            column: Column name where issue was found
            issue_type: Type of issue (missing_value, duplicate, etc.)
            count: Number of rows affected
            action: Action taken (drop_record, impute, set_to_null, etc.)
            reason: Business justification for the action
        """
        self.issues.append({
            'timestamp': datetime.now(),
            'table': table,
            'column': column,
            'issue_type': issue_type,
            'rows_affected': count,
            'action_taken': action,
            'reason': reason
        })
        
    def get_report(self):
        """Return DataFrame with all logged issues."""
        return pd.DataFrame(self.issues)

# Initialize the logger - we'll use this throughout
logger = DataQualityLogger()
```

> ðŸŽ“ **Teaching Note:** The logger is your "black box" for data transformations. Every decision gets recorded with: what happened, how many rows, what action was taken, and why. This is what regulators want to see!

---

### 2.2.3 Layer 1: Schema Validation & Type Coercion

**Goal:** Ensure every column has the correct data type. Convert where possible, log failures.

```python
# Define expected schemas for each table
ACCOUNT_SCHEMA = {
    'account_id': 'string',
    'customer_id': 'string',
    'account_type': 'string',
    'open_date': 'datetime64[ns]',
    'credit_limit': 'float64',
    'status': 'string',
    'branch_code': 'string'
}

def validate_and_coerce_schema(df, schema, table_name, logger):
    """
    Validate and coerce data types according to schema.
    'Coerce' means: Try to convert, if you can't, set to null (don't crash).
    """
    print(f"\n[Layer 1: Schema Validation] Processing {table_name}...")
    df_clean = df.copy()
    
    for col, expected_dtype in schema.items():
        if col not in df_clean.columns:
            continue
            
        # Handle datetime conversion
        if expected_dtype == 'datetime64[ns]':
            original_nulls = df_clean[col].isnull().sum()
            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')
            new_nulls = df_clean[col].isnull().sum()
            failed_conversions = new_nulls - original_nulls
            
            if failed_conversions > 0:
                logger.log_issue(table_name, col, 'type_conversion_failure',
                                failed_conversions, 'set_to_null',
                                'Could not parse as datetime, set to NaT')
                print(f"  {col}: {failed_conversions} invalid dates â†’ NaT")
    
    return df_clean

# Apply schema validation to all tables
accounts_typed = validate_and_coerce_schema(accounts, ACCOUNT_SCHEMA, 'accounts', logger)
transactions_typed = validate_and_coerce_schema(transactions, TRANSACTION_SCHEMA, 'transactions', logger)
balances_typed = validate_and_coerce_schema(balances, BALANCE_SCHEMA, 'balances', logger)
```

**Expected Output:**
```
[Layer 1: Schema Validation] Processing accounts...
  open_date: 478 invalid dates â†’ NaT

[Layer 1: Schema Validation] Processing transactions...
  transaction_date: 4,605 invalid dates â†’ NaT

[Layer 1: Schema Validation] Processing balances...
  balance_date: 21,142 invalid dates â†’ NaT
```

> ðŸ’¡ **Understanding Date Conversion Failures:** The ~50% "invalid dates" aren't garbageâ€”they're valid dates in the wrong format! The data has mixed formats (MM/DD/YYYY and YYYY-MM-DD). When pandas tries to parse mixed formats, it fails on one format, setting those to NaT (null for dates). This simulates real-world systems where data from different sources uses different date conventions.

> ðŸŽ“ **Teaching Note:** "Coerce" means "try to convert, but if you can't, don't crashâ€”just set to null." The `errors='coerce'` parameter is crucial for handling messy real-world data gracefully.

---

### 2.2.4 Layer 2: Handling Missing Data

**Goal:** Decide what to do with null values using column-specific business logic.

**Three Strategies:**
1. **DROP** - If the field is critical (primary keys, required for analysis)
2. **IMPUTE** - If you can infer a reasonable value
3. **KEEP NULL** - If NULL has valid business meaning

```python
def handle_missing_accounts(df, logger):
    """
    Handle missing values in accounts table with column-specific strategies.
    
    Strategy by column:
        - account_id: DROP (primary key, cannot be null)
        - customer_id: DROP (required for identification)
        - account_type: IMPUTE from credit_limit
        - open_date: DROP (required for temporal analysis)
        - credit_limit: KEEP NULL (NULL = not applicable)
        - status: IMPUTE as 'active' (most common)
        - branch_code: KEEP NULL (NULL = online-only account)
    """
    print(f"\n[Layer 2: Missing Data] Processing accounts...")
    df_clean = df.copy()
    initial_count = len(df_clean)
    
    # DROP: Missing account_id (primary key)
    missing_id = df_clean['account_id'].isnull()
    if missing_id.any():
        count = missing_id.sum()
        df_clean = df_clean[~missing_id]
        logger.log_issue('accounts', 'account_id', 'missing_value', count, 
                        'drop_record', 'Primary key cannot be null')
        print(f"  Dropped {count} records with missing account_id")
    
    # ... (similar logic for other columns)
    
    final_count = len(df_clean)
    print(f"  âœ“ Accounts: {initial_count} â†’ {final_count} rows")
    
    return df_clean

# Apply Layer 2 to all tables
accounts_clean = handle_missing_accounts(accounts_typed, logger)
transactions_clean = handle_missing_transactions(transactions_typed, logger)
balances_clean = handle_missing_balances(balances_typed, logger)
```

**Expected Output:**
```
[Layer 2: Missing Data] Processing accounts...
  Dropped 41 records with missing account_id
  Imputed 23 missing account_type values
  Dropped 460 records with missing open_date
  âœ“ Accounts: 1,000 â†’ 499 rows (501 dropped)

[Layer 2: Missing Data] Processing transactions...
  Dropped 169 records with missing IDs
  Dropped 4,526 records with missing transaction_date
  âœ“ Transactions: 9,116 â†’ 4,421 rows (4,695 dropped)

[Layer 2: Missing Data] Processing balances...
  Dropped 21,142 records with missing IDs or dates
  âœ“ Balances: 42,289 â†’ 21,147 rows (21,142 dropped)
```

> ðŸŽ“ **Teaching Note:** Notice we dropped ~50% of the data! This might seem alarming, but it's realistic. In production, you'd investigate WHY so much data is missing before dropping it. The key is: every decision is logged and justified!

---

### 2.2.5 Layer 3: Deduplication & Consistency

**Goal:** Remove duplicates, standardize formats, fix typos, enforce business rules.

```python
def deduplicate_and_standardize_accounts(df, logger):
    """Remove duplicates and standardize text in accounts table."""
    print(f"\n[Layer 3: Deduplication & Consistency] Processing accounts...")
    df_clean = df.copy()
    
    # Remove duplicate account_ids (keep first occurrence)
    duplicates = df_clean['account_id'].duplicated()
    if duplicates.any():
        count = duplicates.sum()
        df_clean = df_clean[~duplicates]
        logger.log_issue('accounts', 'account_id', 'duplicate', count,
                        'drop_record', 'Kept first occurrence of duplicate')
        print(f"  Removed {count} duplicate account_ids")
    
    # Standardize account_type (fix typos)
    type_mapping = {
        'chekcing': 'checking',  # Common typo
        'CHECKING': 'checking',
        'SAVINGS': 'savings'
    }
    for bad_val, good_val in type_mapping.items():
        mask = df_clean['account_type'] == bad_val
        if mask.any():
            df_clean.loc[mask, 'account_type'] = good_val
            logger.log_issue('accounts', 'account_type', 'typo', mask.sum(),
                            'standardize', f'Fixed: {bad_val} â†’ {good_val}')
    
    return df_clean
```

> ðŸ’¡ **Understanding Available vs Ledger Balance:** 
> - **Ledger Balance** = Official account balance (what the bank's books show)
> - **Available Balance** = What you can actually spend (ledger minus holds/pending)
> 
> **Business Rule:** Available can NEVER exceed ledger. If it does, it's a data error!

---

### 2.2.6 Layer 4: Cross-Table Validation (Referential Integrity)

**Goal:** Ensure relationships between tables are valid. Remove orphaned records.

```python
def validate_referential_integrity(accounts, transactions, balances, logger):
    """
    Ensure referential integrity across tables.
    
    Rules:
        1. All transaction.account_id must exist in accounts.account_id
        2. All balance.account_id must exist in accounts.account_id
        3. Transaction dates must be >= account open_date
    """
    print(f"\n[Layer 4: Cross-Table Validation] Enforcing referential integrity...")
    
    valid_account_ids = set(accounts['account_id'].unique())
    
    # Remove orphaned transactions (account doesn't exist)
    orphaned_trans = ~transactions['account_id'].isin(valid_account_ids)
    if orphaned_trans.any():
        count = orphaned_trans.sum()
        transactions = transactions[~orphaned_trans]
        logger.log_issue('transactions', 'account_id', 'orphaned_foreign_key', count,
                        'drop_record', 'Transaction references non-existent account')
        print(f"  Removed {count} orphaned transactions")
    
    return accounts, transactions, balances
```

**Expected Output:**
```
[Layer 4: Cross-Table Validation] Enforcing referential integrity...
  Found 2,362 orphaned transactions (account doesn't exist)
  Removed 47 transactions dated before account opening
  âœ“ Transactions: 4,421 â†’ 2,022 rows
  Found 10,789 orphaned balance records
  âœ“ Balances: 21,147 â†’ 10,021 rows
```

> ðŸ’¡ **Why So Many Orphaned Records?** The orphaned records come from two sources:
> 1. ~10% are intentionally fake (`ACC-99999999`) - simulating data corruption
> 2. ~90% reference accounts that were dropped in Layer 2 (missing data)
> 
> This demonstrates the cascading effect of data cleaning: When you drop an account, you MUST also drop its related transactions and balances.

> ðŸŽ“ **Teaching Note:** Layer 4 = "Clean up the connections." Think of it like checking references on a resumeâ€”if a reference doesn't exist or the dates don't make sense, that's a problem!

---

### 2.2.7 Documentation & Lineage Tracking

Now that we've cleaned the data, let's document everything and create an audit trail.

```python
# Get the complete lineage report
lineage_report = logger.get_report()

print(f"Total issues logged: {len(lineage_report)}")
print(f"Total rows affected: {lineage_report['rows_affected'].sum():,}")

# Save the report
lineage_report.to_csv('data_lineage_report.csv', index=False)
print(f"âœ“ Lineage report saved")
```

### Creating the Data Mart

```python
# Package everything into a clean, documented data mart
os.makedirs('data_mart_clean', exist_ok=True)
os.makedirs('data_mart_clean/data', exist_ok=True)
os.makedirs('data_mart_clean/documentation', exist_ok=True)
os.makedirs('data_mart_clean/logs', exist_ok=True)

# Save cleaned data
accounts_clean.to_csv('data_mart_clean/data/accounts_clean.csv', index=False)
transactions_clean.to_csv('data_mart_clean/data/transactions_clean.csv', index=False)
balances_clean.to_csv('data_mart_clean/data/balances_clean.csv', index=False)

print("âœ“ Data mart complete!")
```

**Final Results:**
```
DATA MART COMPLETE!

Clean data:
  â€¢ 499 accounts (from 1,000 - dropped 50%)
  â€¢ 2,022 transactions (from 9,116 - dropped 78%)
  â€¢ 10,021 balances (from 42,289 - dropped 76%)

Complete audit trail with:
  â€¢ Data lineage report
  â€¢ Data dictionary
  â€¢ Quality improvement metrics
```

---

## 2.3 What We've Accomplished

In this chapter, you've built more than a data cleaning pipelineâ€”you've established a foundation for trustworthy financial analytics.

**You learned to diagnose systematically**, not just by eyeballing data but by running comprehensive quality assessments.

**You built a production-grade pipeline** with four distinct layers:
1. Schema validation that enforces type correctness
2. Missing data handling with documented, column-specific strategies
3. Deduplication and standardization that fixes typos and inconsistencies
4. Cross-table validation that ensures referential integrity

**You documented everything.** Your data dictionary explains what each field means. Your lineage log tracks every transformation. If a regulator asks "how did you get this number?", you can answer with confidence.

---

## Key Takeaways

### 1. Data Quality Has Four Pillars

- **Quality**: Accurate, complete, consistent, timely, valid
- **Lineage**: Traceable from source to output
- **Documentation**: Captures meaning and context
- **Privacy**: Respects regulatory and ethical constraints

These aren't independentâ€”they reinforce each other.

### 2. Context Determines "Good Enough"

| Use Case | Standard |
|----------|----------|
| Exploratory analysis | Lower bar, but document what's missing |
| Model training | High bar, must understand biases |
| Regulatory reporting | Very high bar, complete audit trail |
| Production decisioning | Highest bar, lives at stake |

### 3. Documentation Is Insurance

You pay the cost upfront (time spent documenting) to avoid catastrophic cost later (failed audits, unexplainable model behavior). Every hour spent documenting saves ten hours of forensic debugging.

### 4. Transparency Beats Cleverness

A simple, well-documented pipeline that everyone can understand beats an elegant, opaque one.

### 5. Automate Everything Possible

Manual steps are not reproducible, not auditable, error-prone, and not scalable.

---

## Connecting to the Rest of the Book

The data mart you created in this chapter becomes the foundation for everything that follows:

**Chapter 3: Building the Credit Model**

You'll build credit risk models using clean data. The data quality work you did here directly impacts model performance:
- Missing or incorrect income data â†’ poor credit predictions
- Inconsistent delinquency labels â†’ model learns noise instead of signal

**The lesson**: Garbage in, garbage out.

**Chapter 4: Fairness & Compliance**

The data quality choices you made here have fairness implications:
- Did you drop more records from certain demographic groups?
- Are missing values correlated with protected attributes?
- Do your imputations introduce bias?

**The lesson**: Data quality decisions are ethical decisions.

**Chapter 5: Conclusion & Future**

The documentation and lineage infrastructure you built here scales to model governance:
- Model cards need data cards
- Explainability starts with understanding the input data

**The lesson**: You can't govern what you can't trace.

---

## Teaching Notes

*This section provides guidance for instructors, study groups, and self-directed learners.*

### Learning Objectives

By the end of this chapter, learners should be able to:

**LO1: Diagnostic Skills**
- Systematically identify data quality issues across multiple dimensions
- Distinguish between critical issues (must fix) and cosmetic issues (nice to fix)
- Prioritize data quality work based on downstream impact

**LO2: Technical Implementation**
- Build reproducible data cleaning pipelines using pandas and Python
- Implement schema validation and type coercion
- Handle missing data with appropriate strategies
- Enforce referential integrity across related tables

**LO3: Documentation & Governance**
- Create data dictionaries that capture schema and business meaning
- Implement lineage tracking for all transformations
- Document data quality decisions with clear rationale

**LO4: Business Context**
- Connect data quality practices to regulatory requirements (BCBS 239, SR 11-7)
- Articulate the business impact of data quality failures
- Design data quality rules based on business requirements

### Discussion Questions

1. **The Knight Capital Question:** Why did Knight Capital's test data fail to catch the production bug? What does this teach us about testing data pipelines?

2. **The Documentation Trade-off:** Documentation takes time. When is it worth the investment? When might it be overkill?

3. **The 50% Drop:** We dropped ~50% of our data. When is this acceptable? When would it be a red flag?

4. **NULL Meaning:** We said NULL in `branch_code` means "online account." What could go wrong if this assumption is incorrect?

5. **Fairness Connection:** If missing values are more common for certain demographic groups, what happens when you drop those records? How does this connect to Chapter 4?

### Suggested Exercises

**Exercise 1: Extend the Pipeline (Beginner)**

Add validation for a new business rule: "No transaction amount can exceed the account's credit limit."

**Exercise 2: Simulate Drift (Intermediate)**

Modify the data to include a new issue type (e.g., negative balances). Update your pipeline to detect and handle it.

**Exercise 3: Write a Data Quality Policy (Advanced)**

If you were setting data quality standards for your organization, what would you require? Write a 1-page policy document.

**Exercise 4: Peer Review**

Trade pipelines with a peer and review each other's code. Can you understand their decisions? Would their data pass an audit?

### Assessment Rubric (100 points)

| Dimension | Excellent (90-100%) | Good (80-89%) | Adequate (70-79%) |
|-----------|---------------------|---------------|-------------------|
| **Data Quality (25)** | All issues fixed, minimal data loss | Most issues fixed | Some issues remain |
| **Code Quality (20)** | Clean, modular, runs without errors | Reasonably organized | Functional but messy |
| **Documentation (20)** | Complete lineage, all decisions justified | Most decisions documented | Basic documentation |
| **Business Context (20)** | Rules tied to requirements, regulatory aware | Some business context | Limited context |
| **Validation (15)** | Automated tests, before/after metrics | Basic validation | Minimal checks |

### Key Terms Introduced

| Term | Definition |
|------|------------|
| **BCBS 239** | Basel Committee regulation requiring strong data aggregation capabilities |
| **SR 11-7** | Federal Reserve guidance on model risk management |
| **Data lineage** | The full history of data: where it came from, how it was transformed |
| **Referential integrity** | Ensuring relationships between tables are valid (no orphaned records) |
| **Fit-for-purpose** | Data that is suitable for its intended use |
| **Type coercion** | Converting data from one type to another, handling failures gracefully |

---

*End of Chapter 2*

---

*Next: Chapter 3 â€” Building the Credit Model*
