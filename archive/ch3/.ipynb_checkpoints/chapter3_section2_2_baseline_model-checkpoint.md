# Section 2.2: Baseline Credit Model

You now have a clean dataset with 68 features: 24 original credit attributes plus 44 engineered behavioral features. The data is split temporally (train/val/test), ready for modeling.

This section builds a baseline logistic regression model. Why logistic regression? Three reasons:

1. **Interpretability** - Coefficients have clear meaning, essential for explainability (Section 2.4)
2. **Regulatory acceptance** - Widely used and understood in financial services
3. **Strong baseline** - Often performs surprisingly well on structured data

More complex models (Random Forest, XGBoost) come in Section 2.3. First, we establish a solid, interpretable baseline.

---

## 2.2.1 The Class Imbalance Problem

Before we train anything, we must address a fundamental issue: **class imbalance**.

### Understanding the Problem

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# Load the splits
train = pd.read_csv('model_data/train.csv')
val = pd.read_csv('model_data/val.csv')
test = pd.read_csv('model_data/test.csv')

print("Dataset sizes:")
print(f"  Train: {len(train):,} accounts")
print(f"  Val:   {len(val):,} accounts")
print(f"  Test:  {len(test):,} accounts")

print(f"\nDefault rates:")
print(f"  Train: {train['defaulted'].mean():.1%}")
print(f"  Val:   {val['defaulted'].mean():.1%}")
print(f"  Test:  {test['defaulted'].mean():.1%}")
```

**Expected output:**
```
Dataset sizes:
  Train: 3,799 accounts
  Val:   477 accounts
  Test:  463 accounts

Default rates:
  Train: 5.1%
  Val:   3.4%
  Test:  6.5%
```

**The problem:** Only ~5% of accounts default. If you train a naive model, it can get 95% accuracy by predicting "no default" for everyone!

```python
# Naive baseline: predict everyone pays back
naive_predictions = np.zeros(len(val))
naive_accuracy = (naive_predictions == val['defaulted']).mean()

print(f"\nNaive baseline (predict all non-default):")
print(f"  Accuracy: {naive_accuracy:.1%}")
print(f"  But... detects 0% of actual defaults!")
```

**Output:**
```
Naive baseline (predict all non-default):
  Accuracy: 96.6%
  But... detects 0% of actual defaults!
```

**This is useless.** In credit, failing to detect defaults is catastrophic. We need the model to actually learn the minority class (defaults).

---

## 2.2.2 Handling Class Imbalance

Before trying different approaches, let's prepare our data:

```python
# Separate features from target
feature_cols = [col for col in train.columns if col.startswith('feat_') or 
                col in ['fico_score', 'annual_income', 'debt_to_income_ratio', 
                       'num_delinquencies_24mo', 'credit_utilization_pct', 
                       'age', 'credit_history_months']]

X_train = train[feature_cols]
y_train = train['defaulted']

X_val = val[feature_cols]
y_val = val['defaulted']

# Handle missing values (annual_income can be missing for unemployed/student accounts)
from sklearn.impute import SimpleImputer

print("Preparing data...")
print(f"  Features: {len(feature_cols)}")
print(f"  Train samples: {len(X_train):,}")
print(f"  Val samples: {len(X_val):,}")

missing_train = X_train.isnull().sum().sum()
if missing_train > 0:
    print(f"\n  Found {missing_train} missing values in training data")
    print(f"  Imputing with median...")
    
    imputer = SimpleImputer(strategy='median')
    X_train = pd.DataFrame(
        imputer.fit_transform(X_train),
        columns=X_train.columns,
        index=X_train.index
    )
    X_val = pd.DataFrame(
        imputer.transform(X_val),
        columns=X_val.columns,
        index=X_val.index
    )
    print(f"  ‚úì Imputation complete")

print(f"\n‚úì Data ready for modeling!\n")
```

Three common approaches to handle class imbalance:

---

### Approach 1: Class Weights (Simplest)

Tell the model to penalize default misclassifications more heavily:

```python
# Train with class weights (automatic balancing)
model_weighted = LogisticRegression(
    class_weight='balanced',  # Automatically weights classes inversely to frequency
    max_iter=1000,
    random_state=42,
    n_jobs=1  # Avoid parallel processing issues on some systems
)

model_weighted.fit(X_train, y_train)

# Evaluate
y_pred_weighted = model_weighted.predict(X_val)
print("\nClass-Weighted Model:")
print(classification_report(y_val, y_pred_weighted, 
                           target_names=['No Default', 'Default']))
```

**Expected output:**
```
Class-Weighted Model:
              precision    recall  f1-score   support

  No Default       0.97      0.68      0.80       461
     Default       0.04      0.38      0.07        16

    accuracy                           0.67       477
   macro avg       0.50      0.53      0.44       477
weighted avg       0.94      0.67      0.78       477
```

**How it works:** `class_weight='balanced'` sets weights as `n_samples / (n_classes * np.bincount(y))`. For 5% defaults, default misclassifications are penalized ~19x more. This rescales per-sample loss and gradients, reshaping the loss landscape so errors on rare classes dominate optimization.

**Result:** Recall improves from 0% (naive) to 38%, but precision is only 4% - for every correct default prediction, there are 24 false alarms.

---

### Approach 2: SMOTE (Synthetic Minority Oversampling)

Generate synthetic examples of the minority class by interpolating between existing samples:

```python
from imblearn.over_sampling import SMOTE

# Apply SMOTE to training data
smote = SMOTE(random_state=42)  
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print(f"\nBefore SMOTE:")
print(f"  Total: {len(y_train):,} samples")
print(f"  Defaults: {y_train.sum()} ({y_train.mean():.1%})")

print(f"\nAfter SMOTE:")
print(f"  Total: {len(y_train_smote):,} samples")
print(f"  Defaults: {y_train_smote.sum()} ({y_train_smote.mean():.1%})")

# Train on balanced data
model_smote = LogisticRegression(max_iter=1000, random_state=42, n_jobs=1)
model_smote.fit(X_train_smote, y_train_smote)

# Evaluate
y_pred_smote = model_smote.predict(X_val)
y_prob_smote = model_smote.predict_proba(X_val)[:, 1]

print("\nSMOTE Model:")
print(classification_report(y_val, y_pred_smote, 
                           target_names=['No Default', 'Default']))
```

**Expected output:**
```
Before SMOTE:
  Total: 3,799 samples
  Defaults: 192 (5.1%)

After SMOTE:
  Total: 7,214 samples
  Defaults: 3,607 (50.0%)

SMOTE Model:
              precision    recall  f1-score   support

  No Default       0.96      0.61      0.75       461
     Default       0.03      0.31      0.05        16

    accuracy                           0.60       477
   macro avg       0.49      0.46      0.40       477
weighted avg       0.93      0.60      0.72       477
```

**How it works:** SMOTE creates synthetic minority examples by finding k-nearest neighbors of each minority sample and interpolating between them. This creates new, slightly different examples rather than just duplicating existing ones.

**Result:** Slightly worse than class weights (recall 31% vs 38%), but still much better than naive baseline.

---

### Approach 3: Threshold Adjustment

Use the class-weighted model and adjust the decision threshold to control precision-recall trade-off:

```python
# Reuse model_weighted from Approach 1
# Get probability predictions
y_prob_weighted = model_weighted.predict_proba(X_val)[:, 1]

# Check probability distribution
print(f"\nProbability distribution:")
print(f"  Min: {y_prob_weighted.min():.4f}")
print(f"  Max: {y_prob_weighted.max():.4f}")
print(f"  Mean: {y_prob_weighted.mean():.4f}")
print(f"  Median: {np.median(y_prob_weighted):.4f}")

# Try different thresholds
thresholds = [0.3, 0.5, 0.7]

print("\nThreshold Adjustment:")
print("="*70)
for threshold in thresholds:
    y_pred_threshold = (y_prob_weighted >= threshold).astype(int)
    
    # Calculate metrics
    from sklearn.metrics import precision_score, recall_score, f1_score
    precision = precision_score(y_val, y_pred_threshold, zero_division=0)
    recall = recall_score(y_val, y_pred_threshold, zero_division=0)
    f1 = f1_score(y_val, y_pred_threshold, zero_division=0)
    
    # Count predictions
    n_predicted = y_pred_threshold.sum()
    
    print(f"\nThreshold {threshold:.1f}:")
    print(f"  Predicted defaults: {n_predicted}/{len(y_val)} ({n_predicted/len(y_val)*100:.1f}%)")
    print(f"  Precision: {precision:.3f}")
    print(f"  Recall:    {recall:.3f}")
    print(f"  F1:        {f1:.3f}")
```

**Expected output:**
```
Probability distribution:
  Min: 0.0629
  Max: 0.9379
  Mean: 0.4427
  Median: 0.4296

Threshold Adjustment:
======================================================================

Threshold 0.3:
  Predicted defaults: 418/477 (87.6%)
  Precision: 0.038
  Recall:    1.000
  F1:        0.074

Threshold 0.5:
  Predicted defaults: 152/477 (31.9%)
  Precision: 0.039
  Recall:    0.375
  F1:        0.071

Threshold 0.7:
  Predicted defaults: 14/477 (2.9%)
  Precision: 0.071
  Recall:    0.062
  F1:        0.067
```

**Analysis:**
- **Lower threshold (0.3):** Catches all defaults (100% recall) but flags 88% of applicants with only 4% precision
- **Standard threshold (0.5):** Balanced approach with 38% recall, 4% precision
- **Higher threshold (0.7):** More selective with 7% precision, but only 6% recall (misses most defaults)

**Key finding:** No threshold achieves acceptable performance. The model's probabilities are well-calibrated (mean ~44%), but the features aren't predictive enough. Even best-case precision is only 7%, meaning 13 false alarms per true default.

**This demonstrates the limitation of threshold adjustment:** It can balance precision and recall, but cannot overcome a fundamentally weak model. Section 2.3 will explore better models to improve baseline precision.

---

## 2.2.3 Comparing Approaches

Let's compare all approaches on the validation set:

```python
from sklearn.metrics import roc_auc_score

# Compare the two main approaches
models = {
    'Class Weights': model_weighted,
    'SMOTE': model_smote,
}

print("\nModel Comparison (Validation Set):")
print("="*70)

results = []
for name, model in models.items():
    y_pred = model.predict(X_val)
    y_prob = model.predict_proba(X_val)[:, 1]
    
    # Calculate metrics
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    accuracy = accuracy_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred, zero_division=0)
    recall = recall_score(y_val, y_pred, zero_division=0)
    f1 = f1_score(y_val, y_pred, zero_division=0)
    roc_auc = roc_auc_score(y_val, y_prob)
    
    results.append({
        'Model': name,
        'Accuracy': f"{accuracy:.3f}",
        'Precision': f"{precision:.3f}",
        'Recall': f"{recall:.3f}",
        'F1': f"{f1:.3f}",
        'ROC-AUC': f"{roc_auc:.3f}"
    })
    
results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))

print("\n**Winner:** Class Weights")
print("  ‚úì Higher recall (37.5% vs 31%)")
print("  ‚úì Simpler implementation")
print("  ‚úì No synthetic data needed")
print("  ‚Üí Using Class Weights as baseline for Section 2.3")
```

**Expected output:**
```
Model Comparison (Validation Set):
======================================================================
          Model  Accuracy  Precision  Recall     F1  ROC-AUC
  Class Weights     0.670      0.039   0.375  0.071    0.770
          SMOTE     0.600      0.030   0.312  0.050    0.765

**Winner:** Class Weights
  ‚úì Higher recall (37.5% vs 31%)
  ‚úì Simpler implementation
  ‚úì No synthetic data needed
  ‚Üí Using Class Weights as baseline for Section 2.3
```

**However,** for the remaining sections we'll use the SMOTE model for variety and to demonstrate different techniques. Both perform similarly.

---

## 2.2.4 Credit-Specific Metrics

Standard classification metrics don't tell the full story in credit. Let's calculate industry-standard metrics:

### ROC Curve and AUC

The ROC (Receiver Operating Characteristic) curve shows the trade-off between True Positive Rate and False Positive Rate across all thresholds:

```python
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_val, y_prob_smote)
roc_auc = auc(fpr, tpr)

# Plot
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2,
         label=f'ROC curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('ROC Curve - Credit Default Model')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.savefig('roc_curve.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"ROC-AUC: {roc_auc:.3f}")
```

**Interpretation:**
- **AUC = 0.5:** Random guessing (useless)
- **AUC = 0.524:** Your baseline (barely better than random - 2.4% improvement)
- **AUC = 0.7-0.8:** Acceptable for credit models
- **AUC = 0.8-0.9:** Good performance
- **AUC > 0.9:** Excellent (rare in credit)

**Your result (AUC ‚âà 0.52):** The model struggles to separate defaults from non-defaults. The curve stays very close to the diagonal, indicating weak discrimination power.

---

### Precision-Recall Curve

For imbalanced datasets, the Precision-Recall curve is more informative than ROC:

```python
from sklearn.metrics import precision_recall_curve, average_precision_score

# Calculate PR curve
precision_vals, recall_vals, thresholds_pr = precision_recall_curve(y_val, y_prob_smote)
avg_precision = average_precision_score(y_val, y_prob_smote)

# Plot
plt.figure(figsize=(8, 6))
plt.plot(recall_vals, precision_vals, color='darkorange', lw=2,
         label=f'PR curve (AP = {avg_precision:.3f})')
plt.axhline(y=y_val.mean(), color='navy', linestyle='--', 
            label=f'Baseline (random) = {y_val.mean():.3f}')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall (True Positive Rate)')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Credit Default Model')
plt.legend(loc="best")
plt.grid(alpha=0.3)
plt.savefig('precision_recall_curve.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"Average Precision: {avg_precision:.3f}")
```

**Why PR curve matters:** With only 3.4% defaults, even 100 false positives barely affects False Positive Rate (100/461 = 22%), making ROC look better than it is. But those 100 false positives devastate precision (e.g., 5/(5+100) = 5%), which PR curve reveals.

**Expected Average Precision:** ~0.05-0.10, showing the model barely outperforms random guessing (baseline = 3.4%).

---

### Kolmogorov-Smirnov (KS) Statistic

KS measures the maximum separation between cumulative distributions of defaults and non-defaults:

```python
def calculate_ks_statistic(y_true, y_prob):
    """
    Calculate KS statistic for credit scoring.
    KS = max(TPR - FPR) across all thresholds.
    """
    # Sort by predicted probability
    sorted_indices = np.argsort(y_prob)[::-1]
    y_true_sorted = y_true.iloc[sorted_indices].values
    
    # Calculate cumulative distributions
    n_defaults = y_true_sorted.sum()
    n_non_defaults = len(y_true_sorted) - n_defaults
    
    cum_defaults = np.cumsum(y_true_sorted) / n_defaults
    cum_non_defaults = np.cumsum(1 - y_true_sorted) / n_non_defaults
    
    # KS statistic is max separation
    ks_stat = np.max(cum_defaults - cum_non_defaults)
    ks_index = np.argmax(cum_defaults - cum_non_defaults)
    
    return ks_stat, ks_index

ks_stat, ks_index = calculate_ks_statistic(y_val, y_prob_smote)

print(f"\nKolmogorov-Smirnov (KS) Statistic:")
print(f"  KS = {ks_stat:.3f}")
print(f"  Occurs at position {ks_index} (top {ks_index/len(y_val)*100:.1f}%)")
print(f"\nInterpretation:")
print(f"  KS < 0.2: Poor")
print(f"  KS 0.2-0.4: Acceptable")
print(f"  KS 0.4-0.5: Good")
print(f"  KS > 0.5: Excellent")
```

**Expected KS:** ~0.15-0.25 (poor), indicating weak separation between defaults and non-defaults when accounts are ranked by predicted probability.

---

### Gini Coefficient

Gini is a rescaled AUC, commonly used in credit:

```python
def gini_coefficient(y_true, y_prob):
    """
    Calculate Gini coefficient: Gini = 2*AUC - 1
    Ranges from 0 (no discrimination) to 1 (perfect discrimination)
    """
    auc_score = roc_auc_score(y_true, y_prob)
    gini = 2 * auc_score - 1
    return gini

gini = gini_coefficient(y_val, y_prob_smote)

print(f"\nGini Coefficient:")
print(f"  Gini = {gini:.3f}")
print(f"\nInterpretation:")
print(f"  Gini 0.0: Random (useless)")
print(f"  Gini 0.2: Poor")
print(f"  Gini 0.3-0.5: Acceptable (minimum for production)")
print(f"  Gini 0.5-0.7: Good")
print(f"  Gini > 0.7: Excellent")
```

**Expected Gini:** ~0.05 (4.8% better than random), far below the 0.3 minimum threshold for production credit models.

---

## 2.2.5 Confusion Matrix Analysis

Visualize the types of errors the model makes:

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Calculate confusion matrix
cm = confusion_matrix(y_val, y_pred_smote)

# Plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['No Default', 'Default'],
            yticklabels=['No Default', 'Default'])
plt.title('Confusion Matrix - SMOTE Model')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

# Detailed breakdown
tn, fp, fn, tp = cm.ravel()

print("\nConfusion Matrix Breakdown:")
print(f"  True Negatives (correctly predicted non-default): {tn}")
print(f"  False Positives (wrongly predicted default): {fp}")
print(f"  False Negatives (missed defaults): {fn}")
print(f"  True Positives (correctly predicted default): {tp}")

print("\nError Analysis:")
print(f"  False Positive Rate: {fp/(tn+fp):.1%} (denied good customers)")
print(f"  False Negative Rate: {fn/(fn+tp):.1%} (approved bad customers)")
```

**Business impact:**
- **False Positives (FP):** Good customers wrongly denied ‚Üí Lost revenue
- **False Negatives (FN):** Defaults approved ‚Üí Direct losses

**Expected output:** ~179 false positives (39% FPR) and ~11 false negatives (69% FNR), showing the model struggles with both types of errors.

---

## 2.2.6 Feature Importance Analysis

Understand which features drive predictions:

```python
# Get feature importance from logistic regression coefficients
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'coefficient': model_smote.coef_[0],
    'abs_coefficient': np.abs(model_smote.coef_[0])
}).sort_values('abs_coefficient', ascending=False)

# Top 15 most important features
print("\nTop 15 Most Important Features:")
print("="*70)
print(feature_importance.head(15).to_string(index=False))

# Plot top 10
plt.figure(figsize=(10, 6))
top_features = feature_importance.head(10)
colors = ['red' if coef < 0 else 'green' for coef in top_features['coefficient']]
plt.barh(range(len(top_features)), top_features['coefficient'], color=colors)
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Coefficient (Negative = Lower Default Risk)')
plt.title('Top 10 Most Important Features')
plt.axvline(x=0, color='black', linestyle='--', linewidth=0.5)
plt.grid(alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')
plt.show()
```

**Expected insights:**
- **Negative coefficients** (reduce default risk): higher FICO, higher income, longer credit history
- **Positive coefficients** (increase default risk): more delinquencies, higher DTI, more overdrafts

**Sanity check:** Do these make business sense? If not, investigate data quality or feature engineering issues.

---

## 2.2.7 Initial Fairness Assessment

Before diving deep into fairness (Section 3), let's do a quick check:

```python
# Add predictions to validation set
val_with_preds = val.copy()
val_with_preds['predicted_default'] = model_smote.predict(X_val)
val_with_preds['default_probability'] = model_smote.predict_proba(X_val)[:, 1]

# Check default rates by region
print("\nüìä Fairness Check: Default Rates by Region")
print("="*70)

fairness_stats = val_with_preds.groupby('region').agg({
    'defaulted': ['count', 'mean'],  # Actual default rate
    'predicted_default': 'mean',      # Predicted default rate
    'default_probability': 'mean'     # Average predicted probability
}).round(3)

fairness_stats.columns = ['Count', 'Actual Default Rate', 
                          'Predicted Default Rate', 'Avg Probability']
print(fairness_stats)

# Calculate disparate impact
region_A_rate = val_with_preds[val_with_preds['region']=='A']['predicted_default'].mean()
region_C_rate = val_with_preds[val_with_preds['region']=='C']['predicted_default'].mean()
disparate_impact = region_C_rate / region_A_rate if region_A_rate > 0 else 0

print(f"\n‚ö†Ô∏è Disparate Impact Ratio (Region C / Region A): {disparate_impact:.2f}")
print(f"   (Ratio > 1.25 suggests potential bias)")
```

**Expected output:**
```
üìä Fairness Check: Default Rates by Region
======================================================================
        Count  Actual Default Rate  Predicted Default Rate  Avg Probability
region                                                                      
A         191                0.026                   0.084            0.062
B         191                0.037                   0.110            0.071
C          95                0.053                   0.168            0.095

‚ö†Ô∏è Disparate Impact Ratio (Region C / Region A): 2.00
   (Ratio > 1.25 suggests potential bias)
```

**Red flag!** Region C gets predicted to default at 2x the rate of Region A. This disparate impact will be addressed in Section 3.

---

## 2.2.8 Saving the Baseline Model

```python
import joblib
import os

# Create model directory
os.makedirs('models', exist_ok=True)

# Save the model
joblib.dump(model_smote, 'models/baseline_model_smote.pkl')

# Save feature list (important for production!)
with open('models/feature_list.txt', 'w') as f:
    for feat in feature_cols:
        f.write(f"{feat}\n")

# Save model metadata
metadata = {
    'model_type': 'LogisticRegression',
    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d'),
    'training_samples': len(X_train_smote),
    'n_features': len(feature_cols),
    'imbalance_method': 'SMOTE',
    'val_roc_auc': roc_auc_score(y_val, y_prob_smote),
    'val_avg_precision': average_precision_score(y_val, y_prob_smote),
    'val_ks_statistic': ks_stat
}

import json
with open('models/baseline_model_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

print("\n‚úÖ Model saved to models/")
print(f"   - baseline_model_smote.pkl")
print(f"   - feature_list.txt ({len(feature_cols)} features)")
print(f"   - baseline_model_metadata.json")
```

---

## 2.2.9 Final Evaluation on Test Set

**IMPORTANT:** We only evaluate on test set ONCE at the very end. This is the unbiased estimate of production performance.

```python
# Prepare test data
X_test = test[feature_cols]
y_test = test['defaulted']

# Check for missing values
print(f"Missing values in X_test: {X_test.isnull().sum().sum()}")

# Impute using the same imputer from training
if X_test.isnull().sum().sum() > 0:
    X_test = pd.DataFrame(
        imputer.transform(X_test),  # Use transform, not fit_transform!
        columns=X_test.columns,
        index=X_test.index
    )
    print(f"After imputation: {X_test.isnull().sum().sum()}")

# Get predictions
y_pred_test = model_smote.predict(X_test)
y_prob_test = model_smote.predict_proba(X_test)[:, 1]

# Calculate metrics
from sklearn.metrics import classification_report, roc_auc_score

print("\nüéØ FINAL TEST SET PERFORMANCE")
print("="*70)
print(classification_report(y_test, y_pred_test, 
                           target_names=['No Default', 'Default']))

test_auc = roc_auc_score(y_test, y_prob_test)
test_gini = 2 * test_auc - 1

print(f"\nROC-AUC: {test_auc:.3f}")
print(f"Gini:    {test_gini:.3f}")
```

**Expected output:**
```
üéØ FINAL TEST SET PERFORMANCE
======================================================================
              precision    recall  f1-score   support

  No Default       0.95      0.60      0.74       433
     Default       0.08      0.50      0.14        30

    accuracy                           0.60       463
   macro avg       0.51      0.55      0.44       463
weighted avg       0.89      0.60      0.70       463


ROC-AUC: 0.556
Gini:    0.112
```

**Analysis:**

**Positive findings:**
- ‚úÖ Test performance similar to validation (not overfitting)
- ‚úÖ Recall improved from validation (50% vs 31%)
- ‚úÖ Model generalizes to unseen data

**Concerning findings:**
- ‚ùå Precision only 8% (for every 1 correct default prediction, 11.5 false alarms)
- ‚ùå ROC-AUC 0.556 (only 5.6% better than random)
- ‚ùå Gini 0.112 (far below 0.3 industry minimum)
- ‚ùå Still misses 50% of defaults

**Key insight:** While class balancing (SMOTE) successfully improved recall from 0% to 50%, the 8% precision means we create 11.5 false alarms for every true default detected. This is far below the industry minimum of 15-20% precision.

The baseline model demonstrates that:
1. Class imbalance handling is essential (recall went from 0% ‚Üí 50%)
2. But logistic regression with current features is too weak
3. Section 2.3 must explore more sophisticated models and feature engineering

---

## What You've Accomplished

At this point, you have:

‚úÖ **Handled class imbalance** - Compared three approaches, chose SMOTE
‚úÖ **Trained baseline model** - Logistic regression with 51 features
‚úÖ **Evaluated with credit metrics** - ROC-AUC, KS, Gini, PR curves
‚úÖ **Analyzed feature importance** - Identified key predictors
‚úÖ **Initial fairness check** - Found disparate impact (Region C)
‚úÖ **Saved the model** - Ready for Section 2.3 improvements
‚úÖ **Test set evaluation** - Unbiased performance estimate

**Baseline performance (Test Set):**
- ROC-AUC: 0.556 (poor - barely better than random)
- Recall: 50% (catches half of defaults)
- Precision: 8% (only 1 in 12 predictions correct)
- Gini: 0.112 (far below 0.3 production minimum)
- Disparate Impact: 2.0x (Section 3 addresses this)

**This establishes a clear benchmark:** Section 2.3 must improve precision from 8% to at least 15% while maintaining reasonable recall.

---

## Key Takeaways

Before moving to Section 2.3, make sure you understand:

1. **Class imbalance requires special handling** - Naive models fail to detect rare events (0% recall)

2. **Multiple solutions exist** - Class weights, SMOTE, and threshold adjustment all work, with different trade-offs

3. **Credit uses specialized metrics** - ROC-AUC, KS, Gini are standard in the industry. Don't rely on accuracy alone.

4. **Accuracy is misleading** - A 96% accurate model that catches no defaults is useless in production

5. **There's no free lunch** - Catching more defaults (recall) means more false alarms (lower precision)

6. **Feature importance guides interpretation** - Coefficients should make business sense; if not, investigate

7. **Initial fairness signals matter** - 2x disparate impact warrants investigation (Section 3)

8. **Test set is sacred** - Evaluate only once to get unbiased performance estimate

9. **Weak baseline motivates improvement** - 8% precision and 0.556 AUC clearly need better models

---

## Next Steps

**Section 2.3: Model Improvement**
- Try Random Forest and XGBoost (handle non-linearities better)
- Feature selection and additional feature engineering
- Hyperparameter tuning
- Goal: Improve precision from 8% to 15%+ while maintaining 40%+ recall

**Section 2.4: Model Explainability**
- SHAP values for global and local explanations
- Adverse action notices (regulatory requirement)
- Model cards and documentation

**Section 3: Fairness & Bias Mitigation**
- Measure disparate impact formally
- Apply bias mitigation techniques
- Balance fairness and accuracy trade-offs

---

*(End of Section 2.2)*
